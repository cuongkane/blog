---
title: "Use Claude Code Skill to Efficiently Do Vibe Coding"
description: "Learn how to create and use Claude Code Skills to transform AI-assisted development from a novelty into a reliable engineering practice with consistent, high-quality outputs."
image: "/images/use_claude_code_skill/intro.webp"
date: "2025-01-22"
author: "cuongkane"
published: true
tags: ["claude-code", "ai-coding", "productivity", "developer-tools"]
---

## Introduction: The World with Claude Code

We are living in an exciting era of AI-assisted development. Claude Code has emerged as a powerful CLI tool that transforms how developers interact with codebases. Unlike traditional autocomplete tools, Claude Code understands context, executes commands, reads files, and can even make complex multi-file changes autonomously.

But with great power comes great complexity. As teams adopt Claude Code, a critical question emerges: **How do we ensure consistent, high-quality outputs across different developers and tasks?**

This is where **Skills** become essential.

Think of Claude Code as a highly capable assistant who just joined your team. They're intelligent and eager to help, but they don't know your team's conventions, your coding standards, or your preferred workflows. You could explain everything from scratch each time—or you could create a comprehensive onboarding document that captures all this knowledge.

Skills are exactly that: structured instruction sets that guide Claude Code through specific tasks with precision and consistency.

## Terminology

Before diving deeper, let's clarify the key terms used throughout this post:

| Term | Definition |
|------|------------|
| **Claude Code** | Anthropic's official CLI tool for AI-assisted development. It can read files, execute commands, and make code changes autonomously. |
| **Skill** | A structured set of instructions that guides Claude Code through a specific task. Skills have a name, description, and step-by-step workflow. |
| **Subagent** | An isolated Claude Code process spawned to handle a specific subtask. Subagents have their own context window, preventing the main session from being overwhelmed. |
| **Programmatic Tool Calling** | A technique where skills define templates that invoke Claude Code's tools (like Task, Read, Grep) with specific parameters. This enables automated, repeatable tool usage patterns. |
| **Context Window** | The amount of conversation history and file content Claude Code can "remember" at once. Managing this is critical for complex tasks. |
| **MCP (Model Context Protocol)** | A protocol that allows Claude Code to connect to external services like Jira, Confluence, and databases. |
| **Confirmation Gate** | A checkpoint in a skill workflow where Claude Code pauses and requires explicit user approval before proceeding. |
| **CLAUDE.md** | A special markdown file that Claude Code automatically reads for project-specific conventions, commands, and guidelines. |

## Problem Setup: Why Do We Need Skills?

### The Challenges of AI-Assisted Development

When teams start using Claude Code for complex tasks, several problems emerge:

**1. Inconsistent Outputs**

Without structured guidance, different team members get different results for similar tasks. One developer might get a well-structured implementation following team conventions, while another gets a technically correct but stylistically inconsistent solution.

**2. Knowledge Fragmentation**

Your team has accumulated valuable knowledge:
- Coding conventions and patterns
- Testing standards
- Internal libraries and utilities
- Review and approval workflows

This knowledge lives in documentation, Confluence pages, and tribal knowledge. Claude Code doesn't automatically know about these resources.

**3. Context Window Limitations**

Complex tasks require reading many files, understanding patterns, and planning implementations. Without careful management, the context window gets overwhelmed—leading to forgotten details and inconsistent implementations.

**4. Repetitive Instructions**

For common workflows like "implement a Jira ticket," developers find themselves repeating the same instructions:
- "First read the ticket from Jira"
- "Check the acceptance criteria"
- "Look at similar implementations"
- "Follow our testing patterns"

This is inefficient and error-prone.

### The Scaling Challenge

As your organization grows, these problems multiply:
- More developers using Claude Code
- More conventions to follow
- More internal tools to leverage
- More complex workflows to execute

You need a systematic solution—not just better prompts.

## Claude Skill Solution: Structured Intelligence

### What Are Skills?

Skills are collections of instructions that tell Claude Code what to do for specific tasks. They consist of three critical components:

| Component | Purpose |
|-----------|---------|
| **Name** | Clear identifier that describes the skill's purpose |
| **Description** | Explains when the skill should be used—Claude Code reads this to determine applicability |
| **Instructions** | Step-by-step workflow with phases, checkpoints, and expected outputs |

### The Skill Architecture

```
┌─────────────────────────────────────────────┐
│              SKILL INVOCATION               │
├─────────────────────────────────────────────┤
│  User: "implement HORIZON-1234"             │
│                                             │
│  Claude Code reads skill descriptions       │
│  → Matches implement-python-ticket skill    │
│  → Loads skill instructions                 │
│  → Begins structured workflow               │
└─────────────────────────────────────────────┘
```

## Example: The Implement Python Ticket Skill

Let's examine a real-world skill that transforms how teams implement Jira tickets. This skill guides Claude Code through a systematic workflow from ticket analysis to tested code.

### Workflow Overview

```
┌─────────────────────────────────────────────────────────────┐
│                   IMPLEMENTATION WORKFLOW                   │
└─────────────────────────────────────────────────────────────┘

Phase 0: Authentication & Environment
   ↓ Verify MCP connection, detect project type

Phase 1: Ticket Analysis
   ↓ Fetch ticket, identify gaps in requirements

Phase 2: Requirement Clarification
   ↓ Clarify business requirements (NO code reading yet)
   ↓ ┌─────────────────────────────────────┐
     │    REQUIREMENT CONFIRMATION GATE   │
     │  User confirms understanding       │
     └─────────────────────────────────────┘

Phase 3: Codebase Exploration & Planning (SUBAGENT)
   ↓ ┌─────────────────────────────────────┐
     │  SUBAGENT: Explore → Plan → Approve │
     │                                     │
     │  • Discover patterns from code      │
     │  • Create implementation plan       │
     │  • Present plan to user             │
     │  • Handle revision if needed        │
     │  • Return only when approved        │
     └─────────────────────────────────────┘
   ↓ Save plan to docs/{TICKET_ID}-{slug}.md

Phase 4: Implementation
   ↓ Execute approved plan, match patterns

Phase 5: Testing & Verification
   ↓ Write tests, verify acceptance criteria

Result: Working code + tests + ready for PR
```

### Step-by-Step Walkthrough

#### Phase 0: Authentication & Environment

Before any work begins, the skill verifies prerequisites:

1. **MCP Authentication**: Checks the Atlassian MCP connection. If authentication fails, the skill stops and shows instructions rather than proceeding with incomplete access.

2. **Project Detection**: Identifies the project type from indicators:
   - Django: `manage.py`, `settings.py`, `models.py`
   - Kafka: `kafka/` directory, consumer/producer patterns
   - Generic Python: `pyproject.toml`, standard package structure

3. **Context Loading**: Locates `CLAUDE.md` files and shared utility directories that contain team-specific guidance.

**Why This Matters**: Starting with environment verification prevents wasted effort on tasks that would fail later due to missing access or incorrect assumptions.

#### Phase 1: Ticket Analysis

The skill fetches the Jira ticket and performs a structured analysis:

1. **Extract Information**: Issue key, title, description, acceptance criteria, linked documents
2. **Assess Completeness**: Are the requirements clear? Are acceptance criteria testable?
3. **Categorize Gaps**:
   - Minor (1-2 unknowns): Proceed with clarifying questions
   - Major (3+ unknowns): Suggest requesting a Technical Design document

**Why This Matters**: This prevents the "butterfly effect" where small ambiguities in requirements cascade into significant rework during implementation.

#### Phase 2: Requirement Clarification

This phase focuses purely on **WHAT** needs to be built—not **HOW**.

Key principle: Questions about behavior, edge cases, and business context belong here. Questions about code locations and existing patterns belong in Phase 3.

The skill:
1. Batches clarifying questions (asked all at once for efficiency)
2. Gathers user domain knowledge
3. Consolidates requirements into a summary
4. **GATE**: Requires explicit user confirmation before proceeding

**Why This Matters**: Separating business understanding from implementation details ensures you solve the right problem before investing in exploration.

#### Phase 3: Codebase Exploration & Planning

This is where the skill's architecture shines through **Programmatic Tool Calling**—a technique where the skill defines templates that automatically invoke Claude Code's tools with specific parameters.

**The Technique: Programmatic Tool Calling**

Instead of hoping Claude Code will explore the right files, the skill explicitly defines what tools to call and how:

```
Task(
  description: "Explore and plan for {TICKET_ID}",
  subagent_type: "Plan",
  model: "opus",
  prompt: <structured template with variables>
)
```

This spawns a **subagent**—an isolated Claude Code process with its own context window. The skill passes variables like `{TICKET_ID}`, `{CONSOLIDATED_REQUIREMENTS}`, and `{PATTERN_REFERENCES}` into the template, ensuring the subagent has exactly the context it needs.

**Why Use a Subagent?**
- Exploration requires reading many files
- Keeping this in the main session would bloat the context window
- If the user requests plan revisions, the subagent can adjust without re-reading files
- The subagent's context is isolated—it won't pollute the main session

The subagent workflow:
1. **Explore Codebase**: Find patterns, conventions, utilities, and similar implementations
2. **Load Pattern References**: Fetch established patterns from Confluence documentation
3. **Create Implementation Plan**: Structured plan with files to create/modify, steps, and test strategy
4. **Get User Approval**: Present plan and iterate until approved
5. **Return**: Only exits when the user explicitly approves

The output is saved to `docs/{TICKET_ID}-{slug}.md` as a checkpoint—if implementation needs to restart, this plan serves as the recovery point.

#### Phase 4: Implementation

With an approved plan, implementation becomes straightforward:

1. Follow the approved step order
2. Match discovered patterns and conventions
3. Apply clean code principles
4. Document progress

**Key Principle**: The implementation should fit the codebase, not impose new conventions.

#### Phase 5: Testing & Verification

The skill enforces testing standards:

**Test Structure**:
- AAA pattern (Arrange → Act → Assert)
- Descriptive names: `test_should_{expected_behavior}_when_{scenario}`
- One scenario per test

**What to Test**:
- Behavior, not implementation
- Mock only external boundaries
- Every bug fix requires a regression test

**Quality Targets**:
- 80%+ coverage for new code
- All acceptance criteria verified

### Why This Skill Works

The implement-python-ticket skill succeeds because it:

1. **Separates Concerns**: Requirements → Exploration → Planning → Implementation → Testing
2. **Uses Checkpoints**: Confirmation gates prevent cascading errors
3. **Manages Context**: Subagents isolate heavy exploration from the main session
4. **Encodes Standards**: Testing patterns, code conventions, and approval workflows are built-in
5. **Enables Recovery**: Saved plans allow restart without losing progress

## Claude Skill Best Practices

Based on Anthropic's official recommendations and practical experience, here are the key practices for creating effective skills:

### 1. Invest in Naming and Description

The skill name and description determine whether Claude Code invokes the skill correctly. Be specific:

| Quality | Example |
|---------|---------|
| **Poor** | "python-helper" |
| **Good** | "implement-jira-ticket" |
| **Description** | "Proactively use when user wants to implement a Jira ticket (e.g., 'implement HORIZON-1999', 'work on PROJ-123')" |

### 2. Design Clear Phase Boundaries

Break workflows into distinct phases with specific purposes:

- **Phase separation**: Don't mix requirements gathering with code exploration
- **Clear handoffs**: Define what each phase produces and what the next phase expects
- **Checkpoints**: Add confirmation gates at critical transitions

### 3. Use Subagents for Heavy Exploration

When a task requires reading many files:

```
Task(
  description: "Explore and plan for TICKET-123",
  subagent_type: "Plan",
  model: "opus",
  prompt: <structured template>
)
```

Benefits:
- Isolated context window
- Enables extended thinking
- Supports revision without re-exploration

### 4. Encode Your Team's Knowledge

Reference your actual documentation:
- Confluence pages with coding patterns
- Internal library documentation
- Team conventions and standards

Skills should know where to find authoritative guidance.

### 5. Build in Recovery Points

Save important outputs (like implementation plans) to files:
- Allows restart without losing progress
- Creates audit trail
- Enables handoff between sessions

### 6. Follow the Explore → Plan → Code → Commit Pattern

Anthropic's recommended workflow:

1. **Explore**: Read relevant files without writing code
2. **Plan**: Create documented plan before implementation
3. **Code**: Implement with explicit verification steps
4. **Commit**: Document changes properly

### 7. Be Specific in Instructions

Vague instructions reduce accuracy. Instead of:
> "Add appropriate tests"

Specify:
> "Write tests using AAA pattern. Name tests `test_should_{behavior}_when_{scenario}`. Cover all acceptance criteria behaviors. Target 80%+ coverage."

### 8. Test with Two Claude Sessions

A practical strategy for developing skills is to use **two separate Claude Code sessions**:

```
┌─────────────────────────────────────────────────────────────┐
│                  TWO-SESSION TESTING STRATEGY               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Session 1: SKILL DEVELOPMENT          Session 2: TESTING  │
│  ┌─────────────────────────┐          ┌─────────────────┐  │
│  │ • Write skill code      │          │ • Run example   │  │
│  │ • Edit instructions     │   ───►   │   tasks         │  │
│  │ • Refine based on       │   ◄───   │ • Observe       │  │
│  │   feedback              │          │   behavior      │  │
│  └─────────────────────────┘          └─────────────────┘  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**How it works:**
1. **Session 1** focuses on writing and editing the skill definition
2. **Session 2** tests the skill with real example tasks (e.g., "implement HORIZON-1234")
3. Observe where Claude Code struggles or deviates from expected behavior
4. Return to Session 1 to refine instructions
5. Repeat until the skill performs consistently

This separation prevents the skill-writing context from interfering with skill-testing context, giving you cleaner feedback on how the skill actually performs.

## Lessons from Building the Investigate-Sentry-Issue Skill

While building a skill to investigate Sentry issues, several practical lessons emerged that complement the best practices above:

### 1. Claude is Already Smart—Don't Over-Explain

The initial draft was 326 lines with detailed explanations of what stacktraces are, what Sentry does, and step-by-step MCP tool usage. After applying the "concise is key" principle, it reduced to 147 lines.

**Before (verbose):**
```markdown
## Phase 1: Environment Check

**Intent**: Ensure Sentry MCP is properly configured before starting investigation.

### Steps

1. **Check Sentry MCP connection**: Call `mcp__sentry__whoami` to verify authentication.

2. **If authentication fails**: Stop and display:
   Sentry MCP is not configured or authenticated.
   Please ensure:
   1. Sentry MCP server is configured in your Claude Code settings
   ...
```

**After (concise):**
```markdown
## Phase 1: Environment Check

1. Verify Sentry MCP is authenticated
2. If fails → stop and ask user to configure MCP
3. Check for `CLAUDE.md` for project conventions
```

Claude already knows what MCP authentication means and how to communicate failures. The skill only needs to specify *what* to do, not *how* to explain it.

### 2. Avoid Hardcoding External Dependencies

MCP tool names like `mcp__sentry__whoami` or `mcp__plugin_standard_atlassian__createJiraIssue` are implementation details that can change. Hardcoding them makes the skill fragile.

**Fragile:**
```markdown
Use `mcp__sentry__get_issue_details` with the full URL:
get_issue_details(issueUrl='<full-sentry-url>')
```

**Resilient:**
```markdown
Fetch issue using Sentry MCP
```

Claude knows how to use the available MCP tools. The skill should describe *intent*, not *implementation*.

### 3. Description Drives Skill Discovery

The description field is critical—Claude reads it to decide which skill to trigger. Initially, the skill wasn't being triggered for phrases like "investigate this sentry issue" or "check sentry error."

**Before:**
```yaml
description: Investigates Sentry issues by fetching error details, exploring related code, and generating investigation reports.
```

**After:**
```yaml
description: Proactively use when user wants to investigate a Sentry issue (e.g., "investigate this sentry issue", "check sentry error", "debug this sentry", or provides a sentry.io URL). Fetches error details, explores related code, generates investigation report, and optionally creates Jira bug ticket.
```

The pattern `Proactively use when... (e.g., "trigger phrase 1", "trigger phrase 2")` explicitly tells Claude when to activate the skill.

### 4. Eliminate Duplication Ruthlessly

The initial draft had both a "Workflow Overview" diagram and a "Progress Checklist"—essentially the same information twice.

**Duplicated:**
```markdown
## Workflow Overview
Phase 1: Environment Check     → Verify Sentry MCP
Phase 2: Sentry Analysis       → Fetch issue details
...

## Progress Checklist
- [ ] Phase 1: Verify Sentry MCP connection
- [ ] Phase 2: Fetch and analyze Sentry issue
...
```

**Consolidated:**
```markdown
## Workflow Checklist

Copy and track progress:
- [ ] Phase 1: Environment Check    → Verify Sentry MCP
- [ ] Phase 2: Sentry Analysis      → Fetch issue details
...
```

One element serving both purposes: visual overview AND trackable checklist.

### 5. Review Against the Official Checklist

Anthropic provides a comprehensive checklist for skill quality. Use it as a final review:

| Check | Status |
|-------|--------|
| Description specific with triggers | ✓ |
| Third person description | ✓ |
| Under 500 lines | ✓ (147 lines) |
| Progressive disclosure | ✓ |
| One-level deep references | ✓ |
| Copyable checklist | ✓ |
| No hardcoded dependencies | ✓ |
| Consistent terminology | ✓ |

Multiple review passes caught issues that seemed fine initially—like the hardcoded MCP tool names that only became obvious as "fragile" after explicit review.

### 6. Start with Existing Patterns

Rather than inventing structure from scratch, examine existing skills in your codebase. The `implement-python-ticket` skill provided patterns for:

- Description format: `Proactively use when... (e.g., ...)`
- Phase structure with gates
- Subagent usage for heavy exploration
- Template files in `tasks/` and `templates/` directories

Consistency across skills makes them easier to maintain and understand.

## Conclusion

Claude Code skills transform AI-assisted development from a novelty into a reliable engineering practice. By encoding your team's knowledge, standards, and workflows into structured instructions, you achieve:

- **Consistency**: Every implementation follows the same high-quality process
- **Efficiency**: No more repeating instructions or losing context
- **Scalability**: New team members benefit from accumulated expertise immediately
- **Quality**: Built-in checkpoints and standards prevent common mistakes

The implement-python-ticket skill demonstrates these principles in action: from ticket analysis through tested code, each phase has clear purpose, the workflow manages complexity through subagents, and team standards are enforced automatically.

As your team adopts Claude Code, investing in well-designed skills pays dividends across every task they handle. Start with your most common workflows, encode your best practices, and iterate based on results.

The future of development isn't just AI-assisted—it's AI-augmented with your team's collective intelligence.

---

*This post was prepared for the Engineering Open Space session on Claude Code Skills.*
